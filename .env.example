# Agentic DevOps Pipeline — Environment Configuration
# Copy this file to .env and fill in your values
#
# Two machines run this system:
#   MAC (coffeedev)    — Voice app, Claude Code / Ralph Loop, dev environment
#   GPU (ai-server2)   — Whisper + Ollama inference, demo apps
#
# On Mac: Whisper/Ollama point to ai-server2 (GPU proxy)
# On ai-server2: Whisper/Ollama run locally

# ── Whisper (speech-to-text) ─────────────────────────────────────────
WHISPER_MODEL=small
# Mac: cpu (inference runs on ai-server2 via API)
# ai-server2: cuda
WHISPER_DEVICE=auto

# ── Ollama (intent extraction) ───────────────────────────────────────
OLLAMA_MODEL=qwen2.5-coder-helpful:3b
# Mac: point to ai-server2
# OLLAMA_URL=http://ai-server2:11434
# ai-server2: localhost
OLLAMA_URL=http://localhost:11434
OLLAMA_TIMEOUT=120

# ── Jira (ticket creation + Ralph Loop) ──────────────────────────────
# Required on Mac (pipeline creates tickets from here)
JIRA_URL=https://your-domain.atlassian.net
JIRA_EMAIL=your-email@example.com
JIRA_API_TOKEN=your-api-token-here
JIRA_PROJECT_KEY=DEV

# ── Voice Pipeline Backend ───────────────────────────────────────────
# Mac: lightweight proxy, forwards GPU work to ai-server2
# ai-server2: runs the actual inference
APP_HOST=0.0.0.0
APP_PORT=8000
APP_DEBUG=false
LOG_LEVEL=INFO

# ── Ambiguity loop ───────────────────────────────────────────────────
AMBIGUITY_THRESHOLD=0.3
MAX_CLARIFICATION_ROUNDS=3

# ── Ralph Loop auto-dispatch ─────────────────────────────────────────
AUTO_DISPATCH_LOOP=true
# LOOP_RUNNER_BACKEND_URL=http://localhost:8000
# LOOP_RUNNER_REPO_DIR=/path/to/agentic-devops-loop

# ── Optional: OpenAI Whisper API fallback ────────────────────────────
# OPENAI_API_KEY=sk-...
